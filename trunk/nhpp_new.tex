\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfrag}
\usepackage{usbib}
\usepackage{auto-pst-pdf}
\usepackage{bm}
\usepackage{url}
\usepackage{subfig}

\usepackage[letterpaper,textwidth=14cm]{geometry}

\bibliographystyle{myusmeg-a}
\renewcommand{\citenamefont}[1]{\textsc{\MakeLowercase{#1}}}
\renewcommand{\bibnamefont}[1]{\textsc{#1}}
\renewcommand*{\bibname}{biblography}

\newcommand{\deq}{\triangleq}
\newcommand{\psff}[1]{\input{figures/#1.tex}\includegraphics{figures/#1.eps}}
\newcommand{\cm}[1]{\mathcal{#1}}
\newcommand{\data}{\cm{D}}
\newcommand{\given}{\mid}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\intd}[1]{\,\mathrm{d}#1}

\DeclareMathOperator{\poisson}{Poisson}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Bayesian inference for nonstationary, nonhomogeneous Poisson
  processes}

\author{
  Roman Garnett, Jeff Schneider\\
  {Carnegie Mellon University}\\
  {5000 Forbes Avenue}\\
  {Pittsburgh, Pennsylvania 15213}\\
  \url{{rgarnett, schneide}@andrew.cmu.edu}
  \and
  Michael A Osborne\\
  {University of Oxford}\\
  {Parks Road}\\
  {Oxford OX1 3PJ}\\
  {United Kingdom}\\
  \and
  Richard P Mann\\
  {Uppsala Universitet}\\
  {Uppsala 751\,06}\\
  {Sweden}\\
  \url{rmann@math.uu.se}
}

\begin{document}

\maketitle

\begin{abstract}
  We present a method for performing inference about point processes
  that might undergo sudden changes in their dynamics.  In particular,
  we consider the log-Guassian Cox process, which places a Guassian
  process prior distribution on the logarithm of the intensity
  function of a nonhomogeneous Poisson process.  We model potential
  changepoints in the intensity function using nonsationary
  changepoint covariance functions and will perform fully
  probabilistic changepoint detection by approximating the posterior
  distribution over putative changepoint locations.  We will also
  approximate the full posterior distribution over the intensity
  function, marginalizing out the model hyperparamters, in particular
  the locations of potential changepoints.  We will analyze two
  real-world datasets using these techniques.
\end{abstract}

\section{Nonhomogeneous Poisson Processes and Inference}

A \emph{Poisson process} is a stochastic process that models events
being generated independently from each other with a given rate.  Let
$\cm{X} \subseteq \R^d$ be a Euclidean domain and let $\xi \deq
\sum_{i = 1}^n \delta(x_i)$ be a finite counting measure-valued random
variable on $\cm{X}$, where $\delta$ is the Dirac measure, $n \in \N$
is a natural number-valued random variable, and $\lbrace x_i
\rbrace_{i = 1}^n \subset \cm{X}$ is a random finite set of points.
Let $\lambda\colon \cm{X} \to \R^+$ be an arbitrary nonnegative
function on $\cm{X}$ and let $\lbrace X_i \rbrace_{i = 1}^m$ be a
finite set of disjoint, bounded Borel-measureable subsets of $\cm{X}$.
Define $X \deq \cup_i X_i$.  The Poisson process is characterized by
\begin{align}
  p\bigl(\xi(X) \given \lambda \bigr) 
  &= 
  \prod_{i=1}^m p\bigl(\xi(X_i)\given \lambda \bigr),
  \nonumber
  \\
  p\bigl(\xi(X_i) \given \lambda \bigr)
  &=
  \poisson\left( \textstyle \int_{X_i} \lambda \right),
  \label{realprobability}
\end{align}
where the integral of $\lambda$ is taken with respect to the Lebesgue
measure.

The function $\lambda$ is called the \emph{intensity} function, and
more events are expected to be observed in regions where it is higher.
If $\lambda$ is a constant function, the process is
\emph{homogeneous}; otherwise, it is \emph{nonhomogeneous}.  The input
domain $\cm{X}$ is often taken to be a single temporal dimension,
however spatial and spatiotemporal Poisson process models are also
common.

\subsection{Cox processes}

The \emph{Cox process} provides a common and effective mechanism for
performing inference about nonhomogeneous Poisson processes with an
unknown intensity function.  In a Cox process, the intensity function
is assumed to be generated by a stochastic process separate from the
Poisson process itself.  The generating process can be anything,
although a common approach is the \emph{log-Gaussian} Cox process,
where a Gaussian process prior is placed on the logarithm of intensity
function.  Such a model provides a powerful nonparameteric Bayesian
approach to the problem, and the Gaussian process prior in particular
provides a very flexible framework for modeling the latent intensity.
We will briefly introduce Gaussian processes below then discuss
inference in the context of Poisson processes.

\subsubsection{Gaussian processes}

Gaussian processes provide a simple, flexible framework for performing
Bayesian inference about functions \citep{gpml}.  A Gaussian process
is a distribution on the functions $f\colon \cm{X} \to \R$ with the
property that the distribution of the function values at a finite
subset of points $F \subseteq \cm{X}$ are multivariate Gaussian
distributed. A Gaussian process is completely defined by its first two
moments: a mean function $\mu\colon \cm{X} \to \R$ and a symmetric
positive semidefinite covariance function $K\colon \cm{X} \times
\cm{X} \to \R$.  The mean function describes the overall trend of the
function and is typically set to a constant for convenience.  The
covariance function describes how function values are correlated as a
function of their locations in the domain, thereby encapsulating
information about the overall shape and behavior of the signal.  Many
covariance functions are available to model a wide variety of
anticipated signals.

Suppose we have chosen a Gaussian process prior distribution on the
function $f\colon \cm{X} \to \R$, and a finite set of input points
$\bm{x}$, the prior distribution on $\bm{f} \deq f(\bm{x})$ is
\begin{equation*}
 p(\bm{f} \given \bm{x}, \theta)
 =
 \cm{N}
 \bigl(
   \bm{f};
   \mu(\bm{x}; \theta),
   K(\bm{x}, \bm{x}; \theta)
 \bigr),
\end{equation*}
where $K(\bm{x}, \bm{x}; \theta)$ is the Gram matrix of the points
$\bm{x}$, and $\theta$ is a vector containing any parameters required
of $\mu$ and $K$, which form hyperparameters of the model.

\subsubsection{The log-Gaussian Cox process}

In the log-Gaussian Cox process, we place a Gaussian process prior on
$\nu \deq \log \lambda$:
\begin{equation*}
  p(\nu \given \theta) 
  \deq 
  \cm{GP}\bigl(\nu; \mu(\cdot; \theta), K(\cdot, \cdot; \theta)\bigr).
\end{equation*}
Given some observed events $\data \deq \lbrace x_i \rbrace_{i=1}^n
\subset \cm{X}$, our goal is to find the posterior distribution of
$\nu$ conditioned on those events,
\begin{equation}
  \label{nuposterior}
  p(\nu \given \data)
  =
  \frac
  {
    \int p(\data \given \nu)
    p(\nu \given \theta)
    p(\theta) \intd{\theta}
  }
  {
    \iint p(\data \given \nu)
    p(\nu \given \theta) 
    p(\theta) 
    \intd{\theta} \intd{\nu}
  }.
\end{equation}
The likelihood is given by
\begin{equation}
  \label{hard}
  p(\data \given \nu) 
  =
  \exp\left( -\int_{\cm{V}} \exp \nu \right) \prod_{i = 1}^n \exp \nu(x_i),
\end{equation}
which is unfortunately not tractable due to the integral over $\nu$.
To make things worse, the additional integrals over $\theta$ and $\nu$
in \eqref{hard} are also not tractable.  Therefore we will have to
resort to various approximations.  In this manuscript, we focus on
modeling nonstationary intensity functions with changepoints in their
dynamics rather than inference and will use fairly simple
approximations.

First, we will approximate $p(\data \given \nu)$ via discretization,
suggested by \citet{moller}.  Let $\lbrace X_i \rbrace_{i=1}^m$, $\cup
X_i = \cm{X}$ be a finite partition of $\cm{X}$, where each $X_i$ is
bounded and Borel-measurable. We will assume a constant (log)
intensity on each region $X_i$, which we will notate with $\nu_i$, and
approximate the likelihood with
\begin{equation*}
  p(\data \given \nu) 
  \approx
  \prod_{i=1}^m
  \poisson\bigl(\xi(X_i); m(X_i) \exp \nu_i\bigr),
\end{equation*}
where $m$ is the Lebesgue measure.  This statement agrees with
\eqref{realprobability}, except that we have approximated $\int_{X_i}
\lambda$ with $m(X_i) \exp \nu_i$.  This approixmation has been shown
to be consistent as $\max_i m(X_i) \to 0$ \citep{ghosh}.

Many schemes are available for approximating or sampling from the
posterior $p(\nu \given \data, \theta)$.  In our experiments below, we
will use the simple \emph{Laplace approximation}
\citep{williamsbarber}, which can be efficiently calculated and
performed well for our purposes.  Expectation propagation
\citep{minka} provides a popular alternative.  \citet{adams} gave an
algorithm for sampling exactly from the posterior using Markov chain
Monte Carlo techniques that does not require discretization or other
approximations.  The tradeoff for these more complicated but
potentially more accurate methods is a (sometimes substantial)
increase in computational cost.

Let $\bm{\nu} \deq [\nu_1, \dotsc, \nu_m]^\top$ represent the latent
log intensities on the partition elements $\lbrace X_i \rbrace$.  In
the Laplace approximation, we approximate the posterior $p(\bm{\nu}
\given \data, \theta)$ with a Guassian distribution obtained by taking
a second-order Taylor expansion of the likelihood around its mode.
Let
\begin{align*}
  \hat{\bm{\nu}}
  &\deq 
  \argmax_{\bm{\nu}} p(\bm{\nu} \given \data, \theta), \\
  \Sigma 
  &\deq 
  -\nabla\nabla \log p(\bm{\nu} \given \data, \theta)
  \,
  \Bigr\rvert_{\bm{\nu} = \hat{\bm{\nu}}},
\end{align*}
be the posterior mode of the likelihood and the Hessian of the
negative log likelihood around the mode, respectively.  The Laplace
approximation is 
\begin{equation*}
  p(\nu \given \data, \theta) 
  \approx
  \cm{N}(\hat{\bm{\nu}}, \Sigma^{-1}).
\end{equation*}

\section{Changepoints in Intensity Functions}

In many applications of log-Gaussian Cox processes (e.g.,
\citep{moller, adams}), the distribution of $\lambda$ is assumed to be
translation-invariant (stationary).  Here we are interested in
processes where the intensity function is explicity not assumed to be
stationary---changepoints in the dynamics of the process are localized
and break translation invariance.  Even when modeling a process that
is suspected of having undergone a change, stationary models have been
applied \citep{adams}, and we propose a method for how to avoid that.

The core of our approach will be the application of Gaussian process
covariance functions designed to model sudden changes in their
associated hyperparameters.  These models are based on mechanisms that
can be conveniently ``plugged-into'' standard off-the-shelf stationary
covariances to handle changes in their most-commonly shared features
(such as input and output scales).  

\section{Results}

We have tested the framework described above on two real-world
datasets.  The first is a well-known dataset exhibiting a changepoint
in intensity, and the second is a pair of point processes that undergo
a change in correlation.

\subsection{Coal mining data}

\begin{figure}
  \centering
  \subfloat[][]{
    \psff{posterior}
    \label{posterior}
  }
  \\
  \subfloat[][]{
    \psff{changepoint}
    \label{changepoint}
  }
  \caption{Analysis of the coal-mining data.  \subref{posterior}:
    Inferred intensity function for the process generating the
    disasters.  The rug plot shows the times of the disasters, and the
    posterior distribution over the intensity function is indicated by
    its mean function, shown in dark blue, as well as a pointwise 95\%
    credible interval, shown in light blue.  \subref{changepoint}:
    Posterior distribution over changepoint location.
  }
  \label{coal}
\end{figure}

This dataset (denoted $\data$ below) comprises 190 events over the
time period from March 15, 1851 to March 22, 1962; each represents a
coal mining explosion that killed at least ten people in the United
Kingdom.  These data, first presented by \citet{jarrett}, have been
analyzed several times in the context of nonhomogeneous Poisson
processes, because several legislative acts were passed by the
parliament in an effort to making coal mining safer for workers.  The
events are indicated by the rug plot along the axis of Figure
\ref{posterior}.

We modeled these data as desribed above.  The prior mean of the
Gaussian process on $\nu$ was taken to be the constant zero function,
and the covariance function was chosen to be a nonstationary
adaptation of the squared exponential covariance allowing for a
drastic changepoint in both the input and output scale at a given
time.  The covariance is defined by
\begin{multline*}
  K_{\text{CB}}
  (t, t'; \lambda_1, \lambda_2, \sigma_1, \sigma_2, t_c)
  \\
  \deq
  a(t; \lambda_1, \lambda_2, t_c)
  K_{\text{SE}}
  \bigl(
    u(t; \sigma_1, \sigma_2, t_c),
    u(t'; \sigma_1, \sigma_2, t_c)
    ;
    \lambda = 1, \sigma = 1
  \bigr)
  a(t'; \lambda_1, \lambda_2, t_c),
\end{multline*}
where $t_c$ represents the time of the changepoint, $\lambda_1$ and
$\lambda_2$ represent the output scales before and after the
changepoint, respectively, and $\sigma_1$ and $\sigma_2$ represent the
input scales before and after the changepoint, respectively.  The $a$
and $u$ functions serve to accomplish the desired nonstationarity:
\begin{align*}\label{eq:ufundef}
  a(t; \lambda_1, \lambda_2, t_c) 
  &\deq
  \begin{cases}
    \lambda_1 & t < t_c; \\
    \lambda_2 & t \geq t_c;
  \end{cases}
  \\
  u(t; \sigma_1, \sigma_2, t_c) 
  &\deq
  \begin{cases}
    \frac{t}{\sigma_1} 
    & t < t_c; \\
    \frac{t_c}{\sigma_1} + \frac{t - t_c}{\sigma_2} 
    & t \geq t_c.
  \end{cases}
\end{align*}
Notice that function values are correlated across the changepoint
despite the change in the hyperparameters.

Our model therefore had five associated hyperparameters: the output
scales $\lambda_1$ and $\lambda_2$, the input scales $\sigma_1$ and
$\sigma_2$, and the time of changepoint $t_c$.  Calculating the
posterior distribution $p(\nu \given \data)$ requires marginalizing
these unknown parameters \eqref{nuposterior}.  To accomplish this, we
placed noninformative priors on all five hyperparameters and sampled
from their joint posterior distribution via slice sampling.  For
convenience, the allowed changepoint locations were discretized to a
grid with six months separating each point.  Approximately $4.5 \times
10^6$ samples were taken in total, as well as additional samples that
served as ``burn-in'' for the Makov chain to mix.  The posterior
$p(\nu \given \data)$ was then approximated as a Gaussian process
mixture of the approximate posterior distributions conditioned on the
sampled hyperparamter values.

Figure \ref{coal}\subref{posterior} shows the approximate posterior
distribution $p(\lambda \given \data)$; the distribution was
appropriately transformed from the $\log$ domain.  We also
approximated the posterior distribution over changepoint location
alone, $p(t_c \given \data)$, using Bayesian Monte Carlo and estimates
of the marginal probabilities from the samples described above.  The
result is shown in Figure \ref{coal}\subref{changepoint}.

There seems to be evidence of a changepoint sometime during the period
from 1870--1890.  Previous authors have noted that the parliament of
the United Kingdom passed several acts regulating coal mines, with the
intent of increasing safety for workers.  In particular, the Coal
Mines Regulation Acts of 1872 and 1887 constituted massive reform.
Our analysis here agrees with previous work that these acts were
probably beneficial.  The addition hump near 1950 might be related to
the Mines and Quarries Act, 1954, which imposed further safety
regulation.

We make a few notes on this analysis.  Notice that the posterior
variance of $p(\lambda \given \data)$ is higher in the region from
1870--1890; this corresponds with our uncertainty in $t_c$ and reflects
principled Bayesian analysis.  Furthermore, notice that the posterior
mean prior to the putative changepoint is quite flat.  The
hyperparamter samples with the highest posterior probability in fact
tend to fit the data with an essentially homogeneous Poisson process
during the region from 1851--1870 (by having a very large $\sigma_1$),
followed by a nonhomogeneous process with a much smaller $\sigma_2$.
An additional advantage of our approach is the flexibility afforded by
allowing such nonstationarity in a consistent framework.

\begin{figure}
  \centering
  \hspace*{1em} direct fire incidents 
  \vspace*{1ex} \\
  \psff{direct} \\
  \vspace*{2ex}
  \hspace*{1em} indirect fire incidents
  \vspace*{1ex} \\
  \psff{indirect} \\
\end{figure}

\bibliography{nhpp}

\end{document}
